# WEEK-6_lms
Train multiple machine learning models and evaluate their performance using metrics such as accuracy, precision, recall, and F1-score. Implement hyperparameter tuning techniques like GridSearchCV and RandomizedSearchCV to optimize model parameters. Analyze the results to select the best-performing model.

# 🔍 Model Evaluation & Hyperparameter Tuning

This project demonstrates the complete workflow of training multiple machine learning models, evaluating their performance using key classification metrics, and optimizing them using hyperparameter tuning techniques like `GridSearchCV` and `RandomizedSearchCV`.

## 📂 Dataset

We use the **Breast Cancer Dataset** from `sklearn.datasets`, which contains features computed from digitized images of breast mass tissue samples to predict whether a tumor is malignant or benign.

---

## 📊 Key Features

- Train and compare multiple ML models:
  - Logistic Regression
  - Support Vector Machine (SVM)
  - Random Forest
- Evaluate models using:
  - Accuracy
  - Precision
  - Recall
  - F1-Score
- Perform hyperparameter tuning using:
  - `GridSearchCV` for Random Forest
  - `RandomizedSearchCV` for SVM
- Visualize:
  - Model performance comparison
  - Confusion matrix of the best model

---
## Output

<img width="990" height="590" alt="image" src="https://github.com/user-attachments/assets/602591cb-85a5-4f13-8e15-98d6b44ed178" />

<img width="558" height="490" alt="image" src="https://github.com/user-attachments/assets/eee6781f-adc0-4bc5-ba47-8c7a083e999b" />



## 🧪 Libraries Used

- `scikit-learn`
- `numpy`
- `pandas`
- `matplotlib`
- `seaborn`

Install all dependencies using:
```bash
pip install -r requirements.txt

numpy
pandas
scikit-learn
matplotlib
seaborn




